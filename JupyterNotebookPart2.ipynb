{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_key='XXXXXXXXX'\n",
    "secret='XXXXXXXXX'\n",
    "bucket_name='foobar-bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# make sure pyspark tells workers to use python2 \n",
    "os.environ['AWS_ACCESS_KEY_ID'] = access_key\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = secret\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python2'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-aws:2.7.1,com.amazonaws:aws-java-sdk-pom:1.10.34,com.databricks:spark-csv_2.11:1.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sqlContext = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install this package on the all-spark-notebook docker container, run from within the Docker guest (you can do this via Jupyter Terminal): \n",
    "\n",
    "    /opt/conda/envs/python2/bin/pip install boto\n",
    "    \n",
    "On other platforms the path to `pip` will vary, but the install command is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urlparse import urlsplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the acme CSV (from local disk)\n",
    "\n",
    "Using the Spark SQL [load](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader.load) method, and spark-csv pacakge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acme_file = '/home/jovyan/work/csv/acme_GB_20160923_112850.csv'\n",
    "acme_file = '/home/jovyan/work/csv/acme.csv'\n",
    "\n",
    "acme_df = sqlContext.read.load(acme_file,\n",
    "                                  format='com.databricks.spark.csv',\n",
    "                                  header='true',\n",
    "                                  inferSchema='true')\n",
    "\n",
    "acme_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the acme CSV (from S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn_s3 = boto.connect_s3()\n",
    "bucket = conn_s3.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List bucket contents\n",
    "\n",
    "This bit would drive iterative processing; for now it just picks the first file on the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed/acme_BR_20160803_100000.csv\n",
      "21229079\n",
      "\n",
      "\n",
      "--\n",
      "File to process: processed/acme_BR_20160803_100000.csv\n"
     ]
    }
   ],
   "source": [
    "contents=bucket.list(prefix='processed/acme_BR_20160803_100000')\n",
    "for f in contents:\n",
    "    print f.name\n",
    "    print f.size\n",
    "    acme_file = f.name\n",
    "print \"\\n\\n--\\nFile to process: %s\" % acme_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Testing] Copy file from one bucket to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src=conn_s3.get_bucket('bucket-a')\n",
    "dst=conn_s3.get_bucket('bucket-b')\n",
    "contents=src.list(prefix='source/')\n",
    "for f in contents:\n",
    "    source_name=f.name\n",
    "    target_name = os.path.split(source_name)[1]\n",
    "    print source_name, target_name\n",
    "    dst.copy_key(new_key_name=target_name,\n",
    "                 src_bucket_name=src.name,\n",
    "                 src_key_name=source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Testing] Create a dummy text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = boto.s3.key.Key(bucket)\n",
    "k.key = 'foobar'\n",
    "k.set_contents_from_string('This is a test of S3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Testing] Delete a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = boto.s3.key.Key(bucket)\n",
    "k.key = 'foobar'\n",
    "k.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Testing] Copy a local file to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = boto.s3.key.Key(bucket)\n",
    "k.key = 'acme-sample.csv'\n",
    "k.set_contents_from_filename(acme_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Testing] Get the CSV file from the bucket into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = boto.s3.key.Key(bucket)\n",
    "k.key = acme_file\n",
    "acme_csv = k.get_contents_as_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the CSV from S3 into Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_uri = \"s3n://{}/{}\".format(bucket_name, acme_file)\n",
    "print full_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Loading acme data from: %s' % full_uri\n",
    "acme_df = sqlContext.read.load(full_uri,\n",
    "                                  format='com.databricks.spark.csv',\n",
    "                                  header='true',\n",
    "                                  inferSchema='true')\n",
    "\n",
    "acme_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get country from filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country determined from filename 'acme_BR_20160803_100000.csv' as : BR\n"
     ]
    }
   ],
   "source": [
    "filename=os.path.split(acme_file)[1]\n",
    "import re\n",
    "m=re.search('acme_([^_]+)_.+$', filename)\n",
    "if m is None:\n",
    "    country='NA'\n",
    "else:\n",
    "    country=m.group(1)\n",
    "    \n",
    "print \"Country determined from filename '%s' as : %s\" % (filename,country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add country as a column to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product: string (nullable = true)\n",
      " |-- product_desc: string (nullable = true)\n",
      " |-- product_type: string (nullable = true)\n",
      " |-- supplier: string (nullable = true)\n",
      " |-- date_launched: timestamp (nullable = true)\n",
      " |-- position: string (nullable = true)\n",
      " |-- product_attrib_01: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- product_attrib_02: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- reject_reason: string (nullable = true)\n",
      " |-- country: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acme_df=acme_df.withColumn('country',lit(country))\n",
    "acme_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplicate acme data\n",
    "\n",
    "Based on the url field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acme_deduped_df = acme_df.dropDuplicates(['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original count: 97974\n",
      "Deduplicated count: 96706\n",
      "\n",
      "\n",
      "Number of removed duplicate records: 1268\n"
     ]
    }
   ],
   "source": [
    "orig_count = acme_df.count()\n",
    "deduped_count = acme_deduped_df.count()\n",
    "print \"Original count: %d\\nDeduplicated count: %d\\n\\n\" % (orig_count,deduped_count)\n",
    "print \"Number of removed duplicate records: %d\" % (orig_count - deduped_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe of duplicates for analysis if required \n",
    "\n",
    "Using [subtract](https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.DataFrame.subtract) doesn't work, because the duplicates still have an entry in the de-duplicated set (because they're duplicates, duh). Instead we'll use good ole' fashioned SQL with a `GROUP BY ... HAVING COUNT(*) > 1` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicated URLs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "    acme_deduped_df.select('url').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original list of URLs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "    acme_df.select('url').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acme_df.registerTempTable(\"acme\")\n",
    "\n",
    "duplicates_df = sqlContext.sql(\"SELECT product,product_desc,product_type,supplier,date_launched,position,product_attrib_01,url,product_attrib_02,status,reject_reason,country from acme group by product,product_desc,product_type,supplier,date_launched,position,product_attrib_01,url,product_attrib_02,status,reject_reason,country having count(*)>1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write this to file for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "duplicated_acme_filename='acme_duplicates/duplicates.%s' % filename\n",
    "full_uri = \"s3n://{}/{}\".format(bucket_name, duplicated_acme_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Writing records which are duplicated to %s' % full_uri\n",
    "duplicates_df.coalesce(1).write.save(full_uri,\n",
    "                                     format='com.databricks.spark.csv',\n",
    "                                     header='false',\n",
    "                                     partitionBy='',\n",
    "                                     mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip the url field to give the domain alone, which is the join key to `sites`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out the `urlparse` library for our purpose by taking the url from the first row of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.rittmanmead.com/blog/2016/08/using-apache-drill-with-obiee-12c/\n",
      "www.rittmanmead.com\n"
     ]
    }
   ],
   "source": [
    "sample_url = 'https://www.rittmanmead.com/blog/2016/08/using-apache-drill-with-obiee-12c/'\n",
    "print sample_url\n",
    "\n",
    "print urlsplit(sample_url).netloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the `netloc` column to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works using a [UDF](https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.functions.udf) to apply the `netloc` function to the column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDomain(value):\n",
    "    return urlsplit(value).netloc\n",
    "\n",
    "udfgetDomain = udf(getDomain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses `withColumn` (the column name can be specified). \n",
    "Alternative is `select`, but the column name can't be specified and would need aliasing\n",
    "\n",
    "    acme_deduped_df_with_netloc = acme_deduped_df.select('*',udfgetDomain(acme_deduped_df.url))\n",
    "    \n",
    "`withColumn` therefore seems the tidier solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acme_deduped_df_with_netloc = acme_deduped_df.withColumn(\"netloc\", udfgetDomain(acme_deduped_df.url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the modified dataset (first row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acme_deduped_df_with_netloc.select('date_launched','url','netloc').show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acme_deduped_df_with_netloc.registerTempTable(\"acme\")\n",
    "sqlContext.sql(\"SELECT netloc,count(*) from acme group by netloc order by 2 desc\").show(10)\n",
    "sqlContext.sql(\"SELECT product,netloc,count(*) as cnt from acme group by product,netloc order by cnt desc\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join to sites reference data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the sites CSV\n",
    "\n",
    "Using the Spark SQL [load](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader.load) method, and spark-csv package. \n",
    "\n",
    "_**This should be possible to do directly against Oracle using SparkSQL's JDBC connectivity** - if it was appropriate (i.e. the load wasn't excessive, and freshness of data was important). Alternatively, keep dumping Oracle to flat file and import it into the Spark job each time._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sites_file = \"s3n://{}/{}\".format('foobar-bucket', 'sites.csv')\n",
    "sites_df = sqlContext.read.load(sites_file,\n",
    "                                  format='com.databricks.spark.csv',\n",
    "                                  header='true',\n",
    "                                  inferSchema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sites_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality / Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the first ten rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sites_df.count()\n",
    "sites_df.select('ID','SITE','SITE_RETAIL_TYPE').show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If `SITE_RETAIL_TYPE` is blank, is that in effect NULL?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List out the first ten rows in the reference file that aren't blank `SITE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sites_df.filter(\"SITE != ''\").sort(sites_df.SITE).select('ID','SITE','SITE_RETAIL_TYPE').show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count & list blank `SITE` entries (in case `SITE` is blank but there's other relevant data present that we need to preserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blanks = sites_df.sort(sites_df.SITE).filter(\"SITE = ''\")\n",
    "blanks.show(10)\n",
    "blanks.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count & list blank `SITE_RETAIL_TYPE` entries (in case `SITE_RETAIL_TYPE` is blank but there's other relevant data present that we need to preserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blanks = sites_df.sort(sites_df.SITE).filter(\"SITE_RETAIL_TYPE = ''\")\n",
    "blanks.show(10)\n",
    "blanks.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleanse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove blank SITE entries, and blank SITE_RETAIL_TYPE entries\n",
    "\n",
    "*Maybe we need more subtlety to this, e.g. only drop blank SITE_RETAIL_TYPE if the `SITE_CATEGORY` is `Uncategorized` (so as not to drop useful data)?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sites_before=sites_df.count()\n",
    "print 'Sites before pruning: %d' % sites_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sites_pruned_df = sites_df.filter(\"NOT (SITE ='' OR SITE_RETAIL_TYPE = '')\")\n",
    "sites_after = sites_pruned_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Sites after pruning: %d' % sites_after\n",
    "print 'Sites pruned: %d' % (sites_before-sites_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sites_pruned_df.select('ID','SITE','SITE_CATEGORY','SITE_RETAIL_TYPE').show(20,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the join "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List out the first ten sites in the acme file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acme_deduped_df_with_netloc.sort(acme_deduped_df_with_netloc.netloc).dropDuplicates(['netloc']).select('netloc').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sites_pruned_df.sort(sites_pruned_df.SITE).dropDuplicates(['SITE']).select('SITE').show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_df = acme_deduped_df_with_netloc.join(sites_df,acme_deduped_df_with_netloc.netloc == sites_df.SITE, 'left_outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first ten rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df.select('date_launched','url','netloc','ID','SITE','SITE_RETAIL_TYPE').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 10 matched: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df.filter(merged_df.ID.isNotNull()).select('date_launched','url','netloc','ID','SITE','SITE_RETAIL_TYPE').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 10 unmatched: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df.filter(merged_df.ID.isNull()).select('date_launched','url','netloc','ID','SITE','SITE_RETAIL_TYPE').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records weren't joined to the sites reference? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unmatched_df = merged_df.filter(merged_df.ID.isNull())\n",
    "print \"Unmatched domains: %d\" % unmatched_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the unmatched sites (`netloc`) to file. First do a `coalesce` (to a single partition) so that there's a single file, not hundreds of files in a folder (since the df is partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unmatched_sites_filename='acme_unmatched_sites/unmatched_sites.%s' % filename\n",
    "full_uri = \"s3n://{}/{}\".format(bucket_name, duplicated_acme_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Writing list of sites that are unmatched to %s' % full_uri\n",
    "unmatched_df.select('netloc').coalesce(1).write.save(full_uri,\n",
    "                                                     format='com.databricks.spark.csv',\n",
    "                                                     header='false',\n",
    "                                                     partitionBy='',\n",
    "                                                     mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the joined file to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the joined data to file. First do a `coalesce` (to a single partition) so that there's a single file, not hundreds of files in a folder (since the df is partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acme_enriched_filename='acme_enriched/enriched.%s' % filename\n",
    "full_uri = \"s3n://{}/{}\".format(bucket_name, acme_enriched_filename)\n",
    "\n",
    "print 'Writing enriched acme data to %s' % full_uri\n",
    "\n",
    "merged_df.write.save(full_uri,\n",
    "                     format='com.databricks.spark.csv',\n",
    "                     header='false',\n",
    "                     mode='overwrite')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
